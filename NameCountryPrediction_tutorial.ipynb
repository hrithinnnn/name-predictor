{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrithinnnn/name-predictor/blob/main/NameCountryPrediction_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6mJZbSuyel2"
      },
      "source": [
        "# Introduction\n",
        "---\n",
        "The scope of this tutorial is to introduce the audience to the world of deep learning models and PyTorch as a training framework.  \n",
        "\n",
        "We will use a simple country prediction task to achieve this goal. More details on the task is available below.  \n",
        "\n",
        "# Task description\n",
        "---\n",
        "The task we will try to model today is that of predicting the country of a person from his name, i.e given a name string predict the country where he most probably belongs to.  \n",
        "\n",
        "We will achieve this using a deep learning model using Character-level Convolutional Neural Network (Char-CNN). More details on the model and intuitions as to why this architecture is chosen will be made clear in the following sections.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSzOXQ50yel5"
      },
      "source": [
        "# Dataset\n",
        "---\n",
        "The dataset we will be using is a very simple one used in one of the PyTorch introductory tutorials. It consists of a small zip file containing names of people from 18 different nationalities/regions.  \n",
        "\n",
        "We will use this data to train our model and then test it out.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0m1NoZWyel6"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQvfKGovyel6"
      },
      "outputs": [],
      "source": [
        "!rm data.zip\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip -o data.zip\n",
        "!rm data.zip\n",
        "!mkdir models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFrZciXVyel7"
      },
      "outputs": [],
      "source": [
        "%pip install pytorch_lightning\n",
        "%pip install torch\n",
        "%pip install pandas\n",
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfl5NQAcyel7"
      },
      "source": [
        "## Consolidated imports\n",
        "The below cell consolidates all the required inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxgYWDzLyel8"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from argparse import Namespace\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchinfo\n",
        "from IPython.display import display\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.nn import (\n",
        "    Conv1d,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    LogSoftmax,\n",
        "    MaxPool1d,\n",
        "    NLLLoss,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlavR4Gyyel9"
      },
      "source": [
        "## Seed RNG's\n",
        "In order to make the experiments reproducible we seed the Random Number Generator's in all the libraries used in this training with a fixed seed value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FzazV0Byel9"
      },
      "outputs": [],
      "source": [
        "SEED_VAL = 42\n",
        "random.seed(SEED_VAL)\n",
        "np.random.seed(SEED_VAL)\n",
        "torch.manual_seed(SEED_VAL)\n",
        "torch.cuda.manual_seed_all(SEED_VAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSsedyoEyel-"
      },
      "source": [
        "## Creating our vocab and indices\n",
        "As said before we restrict our input strings to contain only a subset of ASCII characters. So our vocab is as shown in the below and we index the vocab using its position in the below string.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RguOTdUyel-"
      },
      "outputs": [],
      "source": [
        "VOCAB = [\"PAD\"] + list(string.ascii_letters + \" .,;'\")\n",
        "print(\"Vocab size: \" + str(len(VOCAB)))\n",
        "print(\"Index of 'D' in vocab: \" + str(VOCAB.index(\"D\")))\n",
        "PAD_IDX = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LARG9XFtyel_"
      },
      "source": [
        "## Convert names to ASCII\n",
        "Some languages in our input set of names do contain Unicode characters with diacritics. Such names are converted to their equivalent ASCII characters to compromising on pronunciation but reducing the character set for running the training on.  \n",
        "\n",
        "For example, **`Ślusàrski`** is converted to **`Slusarski`**.  \n",
        "\n",
        "The letters we intend to restrict our characters to are **`a-z`**, **`A-Z`** and the special characters **`[space].,;'`**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgWff07wyel_"
      },
      "outputs": [],
      "source": [
        "def convert_unicode_to_ascii(s: str):\n",
        "    return \"\".join(\n",
        "        c\n",
        "        for c in unicodedata.normalize(\"NFD\", s)\n",
        "        if unicodedata.category(c) != \"Mn\" and c in VOCAB\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pufhSD_XyemA"
      },
      "source": [
        "## Read and load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J55_-GIfyemA"
      },
      "outputs": [],
      "source": [
        "files = glob.glob(\"./data/names/*\")\n",
        "\n",
        "category_vs_lines: Dict[str, List[str]] = {}\n",
        "for f in files:\n",
        "    with open(f, \"r\") as fp:\n",
        "        lines: List[str] = fp.readlines()\n",
        "        lines = [convert_unicode_to_ascii(line.strip()) for line in lines]\n",
        "        category = f.split(os.sep)[-1].split(\".\")[0]\n",
        "        category_vs_lines[category] = lines\n",
        "\n",
        "display(\n",
        "    pd.DataFrame(\n",
        "        [{\"Category\": k, \"Count\": len(v)} for k, v in category_vs_lines.items()]\n",
        "    )\n",
        "    .sort_values(by=\"Count\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "CLASSES = sorted(category_vs_lines.keys())\n",
        "NUM_CLASSES = len(category_vs_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggA1OQr0yemA"
      },
      "source": [
        "## Find the mean size of names in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUF7WTC4yemB"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "    [\n",
        "        {\"Value\": v, \"ValueLen\": len(v)}\n",
        "        for _, lines in category_vs_lines.items()\n",
        "        for v in lines\n",
        "    ]\n",
        ")\n",
        "display(df.head())\n",
        "print(\"Mean: {}\".format(df[\"ValueLen\"].mean()))\n",
        "print(\"Median: {}\".format(df[\"ValueLen\"].median()))\n",
        "print(\"Mode: {}\".format(df[\"ValueLen\"].mode()))\n",
        "print(\"Max val: {}\".format(df[\"ValueLen\"].max()))\n",
        "del df\n",
        "\n",
        "SEQUENCE_LEN = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3-a6mAyemB"
      },
      "source": [
        "As the max value of names is 20 we might tend to use that as our maximum sequence length. However on analyzing the mean, median and mode values of lengths of the names in the data we have, we can see that 10 might be a reasonable value for the max sequence length.  \n",
        "\n",
        "So we go ahead and fix our sequence length as 10. We will truncate longer names to 10 characters and pad smaller names with special pad token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXO4R_-8yemC"
      },
      "source": [
        "# Model\n",
        "---\n",
        "We now move on to the model architecture and its creation. We will choose a simple character CNN model with 2 CNN layers and a Pooling layer sandwiched between them.  \n",
        "\n",
        "The final layer will be a linear output layer giving the logits of 18 different categories.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTgQgASvyemC"
      },
      "outputs": [],
      "source": [
        "class NameCountryPredictor(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "        sequence_length: int,\n",
        "        num_classes: int,\n",
        "        dropout: float = 0.1,\n",
        "        learning_rate: float = 0.001,\n",
        "    ):\n",
        "        super(NameCountryPredictor, self).__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        \"\"\"\n",
        "        Input size : N * sequence_length i.e N * 10 here\n",
        "        Output size: N * embed_dim * sequence_length i.e N * 64 * 10 here\n",
        "        \"\"\"\n",
        "        self.embed_layer = Embedding(num_embeddings=len(VOCAB), embedding_dim=128)\n",
        "\n",
        "        \"\"\"\n",
        "        Input size: N * embed_dim * sequence_length i.e N * 64 * 10 here\n",
        "        Output size: Computed dynamically\n",
        "        \"\"\"\n",
        "        self.conv_layers = Sequential(\n",
        "            Conv1d(in_channels=128, out_channels=256, kernel_size=3),\n",
        "            ReLU(),\n",
        "            MaxPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        conv_output_dim = self.__get_conv_output_size((batch_size, sequence_length))\n",
        "\n",
        "        \"\"\"\n",
        "        Input size: N * conv_output_dim\n",
        "        Output_size: N * num_classes\n",
        "        \"\"\"\n",
        "        self.fc_layers = Sequential(\n",
        "            Linear(in_features=conv_output_dim, out_features=256),\n",
        "            ReLU(),\n",
        "            Dropout(p=dropout),\n",
        "            Linear(in_features=256, out_features=num_classes),\n",
        "            LogSoftmax(dim=-1),\n",
        "        )\n",
        "\n",
        "        self.__initialize_weights()\n",
        "\n",
        "    def __initialize_weights(self):\n",
        "        for param in self.parameters():\n",
        "            if param.dim() > 1:\n",
        "                torch.nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def __get_conv_output_size(self, input_size: tuple):\n",
        "        \"\"\"\n",
        "        Method to compute the dimensions of the output after the convolutional layers\n",
        "        \"\"\"\n",
        "        x = torch.ones(input_size, dtype=torch.long)\n",
        "        out = self.encode(x)\n",
        "\n",
        "        # Changing the size of the matrix to retain the last dimension\n",
        "        # and squash all other dimensions to one single dimension\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out_dim = out.size(1)\n",
        "        return out_dim\n",
        "\n",
        "    def forward(self, batch):\n",
        "        out = self.shared_step(input_data=batch)\n",
        "        predicted_class = out.argmax(dim=-1)\n",
        "        return predicted_class\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc, out = self.__predict_and_compute_loss(\n",
        "            batch=batch, batch_idx=batch_idx\n",
        "        )\n",
        "        self.log_dict({\"train_acc\": acc}, prog_bar=True, on_epoch=True, on_step=False)\n",
        "        self.log_dict({\"loss\": loss}, prog_bar=False, on_epoch=True, on_step=True)\n",
        "        return {\"loss\": loss, \"out\": out}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        for name, param in self.named_parameters(prefix=\"c_cnn/\", recurse=True):\n",
        "            self.logger.experiment.add_histogram(name, param, self.current_epoch)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, acc, out = self.__predict_and_compute_loss(\n",
        "            batch=batch, batch_idx=batch_idx\n",
        "        )\n",
        "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, prog_bar=True, on_epoch=True)\n",
        "        return {\"val_loss\": loss, \"val_acc\": acc, \"out\": out}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, acc, _ = self.__predict_and_compute_loss(batch=batch, batch_idx=batch_idx)\n",
        "        self.log_dict(\n",
        "            {\"test_loss\": loss, \"test_acc\": acc}, prog_bar=True, on_epoch=True\n",
        "        )\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        print(\"lr=\", self.learning_rate)\n",
        "        optim = torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "        )\n",
        "        out = {\"optimizer\": optim}\n",
        "        return out\n",
        "\n",
        "    def encode(self, input_data: torch.Tensor):\n",
        "        out = self.embed_layer(input_data)\n",
        "        # N x input_len x embed_dim -> N x embed_dim(channels_in) x input_len\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.conv_layers(out)\n",
        "        return out\n",
        "\n",
        "    def shared_step(self, input_data: torch.Tensor):\n",
        "        out = self.encode(input_data=input_data)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc_layers(out)\n",
        "        return out\n",
        "\n",
        "    def __shared_step(self, batch):\n",
        "        input_data, _ = batch\n",
        "        out = self.shared_step(input_data=input_data)\n",
        "        return out\n",
        "\n",
        "    def __predict_and_compute_loss(self, batch, batch_idx):\n",
        "        _, output_data = batch\n",
        "        predicted_out = self.__shared_step(batch=batch)\n",
        "\n",
        "        loss_fn = NLLLoss()\n",
        "        loss = loss_fn(predicted_out, output_data)\n",
        "\n",
        "        predicted_classes = torch.argmax(input=predicted_out, dim=-1)\n",
        "        batch_accuracy = torch.sum(torch.eq(predicted_classes, output_data)) / float(\n",
        "            torch.numel(output_data)\n",
        "        )\n",
        "        return loss, batch_accuracy, (output_data, predicted_classes)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "model = NameCountryPredictor(BATCH_SIZE, SEQUENCE_LEN, NUM_CLASSES)\n",
        "\n",
        "torchinfo.summary(\n",
        "    model,\n",
        "    input_size=[1, 10],\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
        "    dtypes=[torch.long],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu3BED8iyemD"
      },
      "source": [
        "# Training\n",
        "---\n",
        "After defining the model, we go ahead and implement the training pipeline.  \n",
        "\n",
        "The training is somewhat simplified for us by using the [PytorchLightning](https://www.pytorchlightning.ai/) framework.  \n",
        "\n",
        "We use it's [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) object in order to implement multi device training and learning rate finder options.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaNtW01pyemD"
      },
      "source": [
        "## Decide on whether to use CPU or GPU for training\n",
        "Pytorch Lightning allows us to seamlessly switch our training accelerator from CPU to GPU's and vice versa. We will record the training device and use it as a trainer parameter.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQmYiMDYyemD"
      },
      "outputs": [],
      "source": [
        "gpus_val = torch.cuda.device_count() if torch.cuda.is_available() else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezqqj0j9yemD"
      },
      "outputs": [],
      "source": [
        "class NameCountryDataCollator(object):\n",
        "    def __call__(self, batch):\n",
        "        batch_inp_, batch_op_ = zip(*batch)\n",
        "        stacked_inp = torch.row_stack(list(batch_inp_))\n",
        "        stacked_out = torch.cat(list(batch_op_))\n",
        "        return stacked_inp, stacked_out\n",
        "\n",
        "\n",
        "class NameCountryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def create_split_data_loader(split_df: pd.DataFrame):\n",
        "    data_pairs = split_df.apply(func=tuple, axis=1).to_list()\n",
        "    vectorized_data = [\n",
        "        (\n",
        "            torch.tensor(\n",
        "                inp\n",
        "                if len(inp) == SEQUENCE_LEN\n",
        "                else inp + ([PAD_IDX] * (SEQUENCE_LEN - len(inp))),\n",
        "                dtype=torch.long,\n",
        "            ),\n",
        "            torch.tensor([op], dtype=torch.long),\n",
        "        )\n",
        "        for (inp, op) in data_pairs\n",
        "    ]\n",
        "    return vectorized_data\n",
        "\n",
        "\n",
        "def create_data_loaders():\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"Input\": list(map(lambda x: VOCAB.index(x), list(line[0:SEQUENCE_LEN]))),\n",
        "            \"Output\": CLASSES.index(category),\n",
        "        }\n",
        "        for category, values in category_vs_lines.items()\n",
        "        for line in values\n",
        "    )\n",
        "    display(df.head())\n",
        "    train, val, test = np.split(\n",
        "        df.sample(frac=1, random_state=SEED_VAL),\n",
        "        [int(0.8 * len(df)), int(0.9 * len(df))],\n",
        "    )\n",
        "\n",
        "    data_collator = NameCountryDataCollator()\n",
        "    train_dataloader = DataLoader(\n",
        "        NameCountryDataset(create_split_data_loader(train)),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        NameCountryDataset(create_split_data_loader(val)),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        NameCountryDataset(create_split_data_loader(test)),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "def run_lr_finder(\n",
        "    trainer: Trainer,\n",
        "    training_data_loader: DataLoader,\n",
        "    validation_data_loader: DataLoader,\n",
        "):\n",
        "    lr_finder = trainer.tuner.lr_find(\n",
        "        model=model,\n",
        "        train_dataloader=training_data_loader,\n",
        "        val_dataloaders=validation_data_loader,\n",
        "        min_lr=0.001,\n",
        "        num_training=100,\n",
        "    )\n",
        "    print(lr_finder.results)\n",
        "    new_lr = lr_finder.suggestion()\n",
        "    print(\"New learning rate {}\".format(new_lr))\n",
        "\n",
        "    lr_finder.plot(suggest=True, show=True)\n",
        "    model.learning_rate = new_lr\n",
        "\n",
        "\n",
        "def run_train(args):\n",
        "    (\n",
        "        training_data_loader,\n",
        "        validation_data_loader,\n",
        "        test_data_loader,\n",
        "    ) = create_data_loaders()\n",
        "\n",
        "    callbacks = []\n",
        "\n",
        "    check_pointing = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=5,\n",
        "        filename=\"{epoch}-{val_loss:.4f}-{val_acc:.3f}\",\n",
        "    )\n",
        "\n",
        "    callbacks.append(check_pointing)\n",
        "    callbacks.append(LearningRateMonitor())\n",
        "\n",
        "    tb_logger = TensorBoardLogger(\n",
        "        save_dir=os.path.join(os.getcwd(), \"models\"), name=\"cnn\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        check_val_every_n_epoch=5,\n",
        "        max_epochs=args.epochs,\n",
        "        logger=tb_logger,\n",
        "        callbacks=callbacks,\n",
        "        fast_dev_run=args.is_fast_dev_mode,\n",
        "        gpus=args.gpus,\n",
        "    )\n",
        "\n",
        "    if args.is_lr_mode:\n",
        "        run_lr_finder(\n",
        "            trainer=trainer,\n",
        "            training_data_loader=training_data_loader,\n",
        "            validation_data_loader=validation_data_loader,\n",
        "        )\n",
        "    else:\n",
        "        trainer.fit(\n",
        "            model=model,\n",
        "            train_dataloader=training_data_loader,\n",
        "            val_dataloaders=validation_data_loader,\n",
        "        )\n",
        "        trainer.test(model=model, test_dataloaders=test_data_loader)\n",
        "\n",
        "    return check_pointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0939xlWyemE"
      },
      "source": [
        "## Find the best learning rate\n",
        "We run a [learning rate finder](https://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html#using-lightning-s-built-in-lr-finder) algorithm supported by [PyTorch Lightning](https://pytorch-lightning.readthedocs.io). This is implementation of a PhD. thesis that gives a good initial learning rate for faster convergence of models.  \n",
        "\n",
        "More details on the paper is available in the PyTorch Lightning link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88aOHpBkyemE"
      },
      "outputs": [],
      "source": [
        "args = Namespace(is_fast_dev_mode=False, gpus=gpus_val, is_lr_mode=True, epochs=100)\n",
        "run_train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpf_341ZyemE"
      },
      "source": [
        "## Run training\n",
        "Now we are ready with our newly learnt, learning rate which has been already set in the model instance.  \n",
        "\n",
        "We will now use this model to run the training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBASrq5EyemE"
      },
      "outputs": [],
      "source": [
        "display(model.learning_rate)\n",
        "args = Namespace(is_fast_dev_mode=False, gpus=gpus_val, is_lr_mode=False, epochs=50)\n",
        "model_checkpoint = run_train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICVWXt6GyemF"
      },
      "source": [
        "## Training graphs for loss and accuracy\n",
        "\n",
        "We will now go ahead and see the visualizations of the loss values logged at each time step, both training and validation losses via [Tensorboard](https://www.tensorflow.org/tensorboard/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4EwV5LgyemF"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaRApVEwyemF"
      },
      "source": [
        "# Inference\n",
        "---\n",
        "We will now infer the classes of various inputs as predicted by the best model with respect to validation loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdeY6jTDyemF"
      },
      "outputs": [],
      "source": [
        "def run_inference(model_path: str, input_vals: List[str]):\n",
        "    best_model = NameCountryPredictor.load_from_checkpoint(\n",
        "        model_path,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        sequence_length=SEQUENCE_LEN,\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        re.sub(\n",
        "            \"[^{}]\".format(\"\".join(VOCAB)),\n",
        "            \"\",\n",
        "            convert_unicode_to_ascii(inp)[0:SEQUENCE_LEN],\n",
        "        )\n",
        "        for inp in input_vals\n",
        "    ]\n",
        "    display(inputs)\n",
        "\n",
        "    inp_tensor = torch.tensor(\n",
        "        [\n",
        "            [VOCAB.index(ch) for ch in inp]\n",
        "            + (\n",
        "                [PAD_IDX]\n",
        "                * ((SEQUENCE_LEN - len(inp)) if len(inp) < SEQUENCE_LEN else 0)\n",
        "            )\n",
        "            for inp in inputs\n",
        "        ],\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    predicted_classes = model(inp_tensor)\n",
        "    display(\n",
        "        pd.DataFrame(\n",
        "            [\n",
        "                {\"Name\": name, \"Category\": CLASSES[predicted_class]}\n",
        "                for name, predicted_class in zip(input_vals, predicted_classes.tolist())\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "infer_inps = [\n",
        "    \"Alexis\",\n",
        "    \"Aimée Leigh\",\n",
        "    \"Vasily Grigoryevich Zaitsev\",\n",
        "    \"Joaquin\",\n",
        "    \"Émer\",\n",
        "    \"Aleksander\",\n",
        "]\n",
        "run_inference(model_checkpoint.best_model_path, infer_inps)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a82c866d114c775dad3b96351b4c4a9827754620040b4a7ca0f805bf85781af7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 ('ssn_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of NameCountryPrediction_SS_tutorial",
      "provenance": [],
      "collapsed_sections": [
        "DSzOXQ50yel5",
        "OXO4R_-8yemC",
        "qu3BED8iyemD",
        "HaRApVEwyemF"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}